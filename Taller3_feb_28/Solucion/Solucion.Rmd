---
header-includes:
- \usepackage{float}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish, es-tabla]{babel}\decimalpoint
- \usepackage{amsmath}
output:
  pdf_document: default
  html_document: default
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: "es"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
library(knitr)
library(kableExtra)
library(tidyverse)
```


**Estadística II - Taller 03$\hspace{1.6cm}$Semestre: 2023-01**

**Profesores: Francisco Javier Rodríguez Cortés, Julieth Verónica Guarín Escudero**

**Monitor: Matheo Muñoz Betancur**

$\rule{6.5in}{1pt}$

Este taller se divide en dos secciones, en la primera se trabajará lo relacionado
a la validación del modelo. Posterior a esto, se considera un ejercicio en el que
se realiza la prueba de falta de ajuste a un modelo.

En primer lugar considere el siguiente conjunto de datos.

```{r chunck, echo=F}
gen_dat <- function(n, seed = 7){
  varianza <- 16
  set.seed(seed)
  x <- rep(runif(n=floor(n/2)+1, min=-5, max=6),2)[sample(2*floor(n/2)+2,n)]
  media <- 4 - 6 * x + 2 * x^2
  set.seed(seed^2)
  y <- rnorm(n=n, mean=media, sd=sqrt(varianza))
  marco_datos <- data.frame(y=y, x=x)
  return(marco_datos)
}

datos <- gen_dat(75)

datos[1:5, ] %>% 
  kable(booktab = T, caption = "Presentación de los datos",
        longtable = T, align = "c") %>%
  kable_styling(latex_options = "HOLD_position")
```

El día de hoy, la misión será realizar los siguientes ejercicios, claro está,
haciendo uso de **R**.


1. Genere la base de datos que se muestra previamente usando el siguiente código.

```{r gendatos, eval=F}
gen_dat <- function(n, seed = 7){
  varianza <- 16
  set.seed(seed)
  x <- rep(runif(n=floor(n/2)+1, min=-5, max=6),2)[sample(2*floor(n/2)+2,n)]
  media <- 4 - 6 * x + 2 * x^2
  set.seed(seed^2)
  y <- rnorm(n=n, mean=media, sd=sqrt(varianza))
  marco_datos <- data.frame(y=y, x=x)
  return(marco_datos)
}

datos <- gen_dat(75)
```


2. Ajuste el modelo de regresión lineal simple
$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \ \varepsilon_i \overset{iid}{\sim} N(0, \sigma^2); \ 1 \leq i \leq 75$$
3. Determine que parámetros son significativos y cuales no en el modelo, hágalo de 
manera rápida aprovechando alguna de las funciones de `R` usadas hasta el momento. \textbf{Nota:} Asuma que se cumple el supuesto de independencia de los residuales. 
4. Extraiga los residuales del modelo y verifique que estos tengan media igual a 0.
5. Determine si los residuales tienen varianza constante, argumente por qué esto
es o no es así, además, si nota algún patrón o algo que considere anormal, coméntelo.
6. Evalúe el supuesto de normalidad de los residuales, hágalo usando un gráfico cuantil - cuantil y finalmente una prueba de hipótesis.
7. Realice la prueba de falta de ajuste para los datos
del modelo $$y_i = \beta_0 + \beta_1 xi + \varepsilon_i, \ \varepsilon_i \overset{iid}{\sim} N(0, \sigma^2); \ 1 \leq i \leq 75$$
para ello use la función `rsm` del paquete `rsm`.

8. Con la base de datos del archivo decaimiento.xlsx, haga un análisis
de si se puede ajustar un modelo, bien sea lineal o intrínsecamente lineal,
escriba el modelo y cúales son sus supuestos, reporte los coeficientes
estimados e interprételos.

\textbf{Nota:} se propone como ejercicio realizar la validación del modelo

\section*{Solución}

\subsection*{Ejercicio 1}

```{r}
gen_dat <- function(n, seed = 7){
  varianza <- 16
  set.seed(seed)
  x <- rep(runif(n=floor(n/2)+1, min=-5, max=6),2)[sample(2*floor(n/2)+2,n)]
  media <- 4 - 6 * x + 2 * x^2
  set.seed(seed^2)
  y <- rnorm(n=n, mean=media, sd=sqrt(varianza))
  marco_datos <- data.frame(y=y, x=x)
  return(marco_datos)
}

datos <- gen_dat(75) #Generando los datos
```
\subsection*{Ejercicio 2}
```{r}
mod <- lm(y ~ x, data = datos)
```
\subsection*{Ejercicio 3}
```{r}
summary(mod)
```
Ambos parámetros son individualmente significativos a un nivel de significancia $\alpha = 0.05$.

\subsection*{Ejercicio 4}

```{r}
residuales <- residuals(mod) #extrayendo los residuales
mean(residuales) #la media de los residuales da 0
```
Esto se da producto de utilizar mínimos cuadrados ordinarios, del cual

\begin{align*}
  \sum_{i=1}^n e_i = \sum_{i=1}^n (Y_i - \hat{\beta_0} - \hat{\beta_1}X_i) &= \sum_{i=1}^n (Y_i - \bar{Y} + \hat{\beta_1} \bar{X} - \hat{\beta_1}X_i) = \\
  &\sum_{i=1}^n [(y_i-\bar{Y})-\hat{\beta_{1}}(X_i-\bar{X})] = 0
\end{align*}

Basado en los residuales, los errores tienen media 0:
$$\sum_{i=1}^n \frac{e_i}{n} = 0$$
\subsection*{Ejercicio 5}

```{r}
ajustados <- fitted(mod) #valores ajustados
plot(ajustados, residuales, pch=20)
```

Es clara la forma en U de los residuales vs valores ajustados, lo que da indicio de falta de ajuste, sin embargo no hay evidencia suficiente en contra de varianza constante, por lo que para este caso se considera que se cumple el supuesto.

\subsection*{Ejercicio 6}

```{r}
qqnorm(residuales, pch=20)
qqline(residuales)
```

No se observa una distribución de normalidad en la gráfica de comparación de cuantiles, tanto por el patrón que esta presenta como por tener colas más pesadas, se plantea entonces la siguiente prueba de hipótesis:
$$
\begin{cases}
  H_0: \text{Los }\varepsilon_i \text{ distribuyen normal} \\
  H_a: \text{Los }\varepsilon_i \text{ no distribuyen normal}
\end{cases}
$$

La siguiente es la prueba de normalidad shapiro wilk para la anterior prueba de hipótesis:
```{r}
shapiro.test(residuales)
```
Como Val-P$< \alpha$ entonces se rechaza la hipótesis nula en la que los datos provienen de una población normal, por lo que se acepta la hipótesis en la que no lo hacen.

\subsection{Ejercicio 7}

```{r}
#install.packages("rsm") instalando el paquete necesario
#graficando los datos
with(datos, plot(x, y, pch=20))
```
Es posible hacer prueba de falta de ajuste pues existen valores repetidos para datos en $X$. Así, la prueba de hipótesis es:
$$
\begin{cases}
  H_0: \text{E}[y|x_i] = \beta_0 + \beta_1x_i \\
  H_a: \text{E}[y|x_i] \neq \beta_0 + \beta_1x_i
\end{cases}
$$
En **R**:
```{r}
library(rsm)
mod.falta.ajuste <- rsm(y ~ FO(x), data = datos)
summary(mod.falta.ajuste)
```

Como el valor P es demasiado pequeño, entonces a cualquier nivel de significancia $\alpha$ se rechaza la hipótesis nula, por lo que el modelo lineal tiene falta de ajuste (hecho que se evidencia gráficamente), también se observa que a un nivel de significancia $\alpha=0.05$, el modelo es significativo, sin embargo la variabilidad de la respuesta explicada por la regresión es de solamente 16.38\%.

\subsection{Ejercicio 8}

```{r}
datos.nuevos <- readxl::read_xlsx("decaimiento.xlsx")
with(datos.nuevos, plot(t, sustancia, pch=20))
```

Es claro que el modelo no es lineal, sin embargo tiene un comportamiento exponencial multiplicativo, por lo que se considera $Y=\beta_0e^{\beta_1X}\varepsilon$ y se ajusta $Y^*=\beta_o^* + \beta_1X + \varepsilon^*$ con $Y^*=\ln(Y)$. En **R**:
```{r}
mod.transform <- lm(log(sustancia) ~ t, data = datos.nuevos)
summary(mod.transform)
```
Los parámetros son significativos y se leen igual, pero se concluye en función del logaritmo natural de la respuesta. El $R^2$ de un modelo transformado no es comparable con el $R^2$ del modelo sin transformar en el caso que se transforme la respesta $Y$, como en este ejercicio. Se deben analizar los residuales del modelo transformado, pues $\varepsilon_i^* \stackrel{\text{iid}}{\sim} \text{N    ormal}$.

